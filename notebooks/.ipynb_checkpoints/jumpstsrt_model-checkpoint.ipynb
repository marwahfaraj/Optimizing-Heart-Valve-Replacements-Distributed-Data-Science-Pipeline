{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875e55fb-caa7-45f6-8ee5-2320d1bdef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label distribution:\n",
      "cardio\n",
      "0    35021\n",
      "1    34979\n",
      "Name: count, dtype: int64\n",
      "Final label distribution:\n",
      "cardio\n",
      "0    35021\n",
      "1    34979\n",
      "Name: count, dtype: int64\n",
      "Split complete - Train: (49000, 16), Test: (21000, 16)\n",
      "Last column sample values: {'0.000000', '2.000000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: cardio-xgboost-final-2025-03-31-05-50-21-881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with verified data...\n",
      "2025-03-31 05:50:24 Starting - Starting the training job...\n",
      "2025-03-31 05:50:39 Starting - Preparing the instances for training...\n",
      "2025-03-31 05:51:02 Downloading - Downloading input data...\n",
      "2025-03-31 05:51:52 Downloading - Downloading the training image......\n",
      "2025-03-31 05:52:54 Training - Training image download completed. Training in progress.\n",
      "2025-03-31 05:52:54 Uploading - Uploading generated training model\u001b[34m[2025-03-31 05:52:45.462 ip-10-0-223-231.ec2.internal:8 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Failed to parse hyperparameter eval_metric value error to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Train matrix has 49000 rows and 15 columns\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.599 ip-10-0-223-231.ec2.internal:8 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.600 ip-10-0-223-231.ec2.internal:8 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.601 ip-10-0-223-231.ec2.internal:8 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.602 ip-10-0-223-231.ec2.internal:8 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.602 ip-10-0-223-231.ec2.internal:8 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2025-03-31:05:52:45:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.26310\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.710 ip-10-0-223-231.ec2.internal:8 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2025-03-31 05:52:45.712 ip-10-0-223-231.ec2.internal:8 INFO hook.py:486] Hook is writing from the hook with pid: 8\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.26178\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 122 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.25853\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.25771\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 124 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.25677\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 126 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.25649\u001b[0m\n",
      "\u001b[34m[05:52:45] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.25659\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.25625\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 120 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.25484\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.25398\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.25376\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.25312\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.25163\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 114 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.25120\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.25045\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 110 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.24947\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.24916\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.24874\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.24843\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.24796\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.24759\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.24733\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 24 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.24716\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.24667\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.24618\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.24594\u001b[0m\n",
      "\u001b[34m[05:52:46] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.24557\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.24574\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.24531\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 112 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.24480\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.24404\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.24359\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.24288\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 40 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.24274\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 108 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.24206\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 68 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.24202\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 48 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.24184\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.24143\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.24124\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 16 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.24122\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.24086\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.24008\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.23971\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 118 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.23898\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.23865\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.23843\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.23802\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 54 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.23775\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.23704\u001b[0m\n",
      "\u001b[34m[05:52:47] INFO: ../src/tree/updater_prune.cc:101: tree pruning end, 98 extra nodes, 0 pruned nodes, max_depth=6\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.23633\u001b[0m\n",
      "\n",
      "2025-03-31 05:53:07 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "import io\n",
    "\n",
    "# Step 1: Setup\n",
    "role = get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "bucket = \"cardiovale-solutions-datascience-pipeline\"\n",
    "train_key = \"xgb/manual/train/train.csv\"\n",
    "\n",
    "# Step 2: Load data with extreme validation\n",
    "s3_client = boto3.client(\"s3\")\n",
    "source_key = \"feature-store/cardio/cardio-feature-group-22-21-14-34/autopilot_input.csv\"\n",
    "response = s3_client.get_object(Bucket=bucket, Key=source_key)\n",
    "df = pd.read_csv(response[\"Body\"])\n",
    "\n",
    "# Step 3: Nuclear option for label cleaning\n",
    "print(\"Original label distribution:\")\n",
    "print(df[\"cardio\"].value_counts())\n",
    "\n",
    "# Convert to numeric, drop NA, then force binary\n",
    "df[\"cardio\"] = pd.to_numeric(df[\"cardio\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"cardio\"])\n",
    "df[\"cardio\"] = np.where(df[\"cardio\"] > 0.5, 1, 0).astype(int) \n",
    "\n",
    "# Final validation\n",
    "assert set(df[\"cardio\"].unique()) == {0, 1}, f\"Invalid labels found: {df['cardio'].unique()}\"\n",
    "print(\"Final label distribution:\")\n",
    "print(df[\"cardio\"].value_counts())\n",
    "\n",
    "# Step 4: Prepare features\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# Step 5: Move label to last position\n",
    "label_col = \"cardio\"\n",
    "cols = [c for c in df.columns if c != label_col] + [label_col]\n",
    "df = df[cols]\n",
    "\n",
    "# Ensure label is the first column (required for CSV with no header)\n",
    "label_col = \"cardio\"\n",
    "cols = [label_col] + [col for col in df.columns if col != label_col]\n",
    "df = df[cols]\n",
    "\n",
    "# Step 6: Create train/test split\n",
    "train_df = df.sample(frac=0.7, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "print(f\"Split complete - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "# Step 7: Save to CSV with explicit formatting\n",
    "csv_buffer = io.StringIO()\n",
    "train_df.to_csv(csv_buffer, index=False, header=False, float_format=\"%.6f\")\n",
    "csv_content = csv_buffer.getvalue()\n",
    "\n",
    "# Verify the last column\n",
    "last_col_vals = [row.split(\",\")[-1].strip() for row in csv_content.split(\"\\n\") if row]\n",
    "print(f\"Last column sample values: {set(last_col_vals[:100])}\")\n",
    "\n",
    "# Upload with proper content type\n",
    "s3_client.put_object(\n",
    "    Bucket=bucket,\n",
    "    Key=train_key,\n",
    "    Body=csv_content,\n",
    "    ContentType=\"text/csv\"\n",
    ")\n",
    "\n",
    "# Step 8: Configure training\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")  \n",
    "\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=xgboost_container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/xgb/manual/output/\",\n",
    "    sagemaker_session=session,\n",
    "    base_job_name=\"cardio-xgboost-final\"\n",
    ")\n",
    "\n",
    "# Simplified hyperparameters\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    num_round=50,\n",
    "    eval_metric=\"error\",\n",
    "    verbosity=2  # Increased logging\n",
    ")\n",
    "\n",
    "# Step 9: Train with File mode\n",
    "print(\"Starting training with verified data...\")\n",
    "xgb_estimator.fit(\n",
    "    {\"train\": TrainingInput(\n",
    "        f\"s3://{bucket}/{train_key}\",\n",
    "        content_type=\"text/csv\",\n",
    "        input_mode=\"File\",\n",
    "        distribution=\"ShardedByS3Key\"  # Better for large files\n",
    "    )},\n",
    "    wait=True,\n",
    "    logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "196086ef-4196-4a1a-b222-a414ff1f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: cardio-xgboost-final-2025-03-31-06-02-28-509\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::786782285170:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-1:786782285170:model/cardio-xgboost-final-2025-03-31-06-02-28-509 with an explicit deny in an identity-based policy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Deploying The Predictor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xgb_predictor \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.m5.large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/estimator.py:1684\u001b[0m, in \u001b[0;36mEstimatorBase.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, serverless_inference_config, async_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m model\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m   1680\u001b[0m tags \u001b[38;5;241m=\u001b[39m update_inference_tags_with_jumpstart_training_tags(\n\u001b[1;32m   1681\u001b[0m     inference_tags\u001b[38;5;241m=\u001b[39mformat_tags(tags), training_tags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m   1682\u001b[0m )\n\u001b[0;32m-> 1684\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/model.py:1695\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# existing single model endpoint path\u001b[39;00m\n\u001b[0;32m-> 1695\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_sagemaker_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1702\u001b[0m     serverless_inference_config_dict \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1703\u001b[0m         serverless_inference_config\u001b[38;5;241m.\u001b[39m_to_request_dict() \u001b[38;5;28;01mif\u001b[39;00m is_serverless \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m     )\n\u001b[1;32m   1705\u001b[0m     production_variant \u001b[38;5;241m=\u001b[39m sagemaker\u001b[38;5;241m.\u001b[39mproduction_variant(\n\u001b[1;32m   1706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1707\u001b[0m         instance_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         routing_config\u001b[38;5;241m=\u001b[39mrouting_config,\n\u001b[1;32m   1715\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/model.py:980\u001b[0m, in \u001b[0;36mModel._create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags, serverless_inference_config, accept_eula, model_reference_arn)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m resolve_nested_dict_value_from_config(\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv,\n\u001b[1;32m    968\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    969\u001b[0m     MODEL_CONTAINERS_PATH,\n\u001b[1;32m    970\u001b[0m     sagemaker_session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session,\n\u001b[1;32m    971\u001b[0m )\n\u001b[1;32m    972\u001b[0m create_model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    973\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    974\u001b[0m     role\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    978\u001b[0m     tags\u001b[38;5;241m=\u001b[39mformat_tags(tags),\n\u001b[1;32m    979\u001b[0m )\n\u001b[0;32m--> 980\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcreate_model_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:3976\u001b[0m, in \u001b[0;36mSession.create_model\u001b[0;34m(self, name, role, container_defs, vpc_config, enable_network_isolation, primary_container, tags)\u001b[0m\n\u001b[1;32m   3973\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3974\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 3976\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_model_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:6514\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   6497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   6498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6499\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6502\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   6503\u001b[0m ):\n\u001b[1;32m   6504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   6505\u001b[0m \n\u001b[1;32m   6506\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6512\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   6513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:3964\u001b[0m, in \u001b[0;36mSession.create_model.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   3962\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreateModel request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m   3963\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3964\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   3966\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the CreateModel operation: User: arn:aws:sts::786782285170:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateModel on resource: arn:aws:sagemaker:us-east-1:786782285170:model/cardio-xgboost-final-2025-03-31-06-02-28-509 with an explicit deny in an identity-based policy"
     ]
    }
   ],
   "source": [
    "# Deploying The Predictor\n",
    "xgb_predictor = xgb_estimator.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df6b9857-09f4-4930-82b4-ac9d42c2cb76",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m test_array \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcardio\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Set the serializer for converting input to CSV format\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mxgb_predictor\u001b[49m\u001b[38;5;241m.\u001b[39mserializer \u001b[38;5;241m=\u001b[39m CSVSerializer() \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m xgb_predictor\u001b[38;5;241m.\u001b[39mpredict(test_array)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# Use test_df (or whatever variable holds your test data) instead of test\n",
    "test_array = test_df.drop(['cardio'], axis=1).values\n",
    "\n",
    "# Set the serializer for converting input to CSV format\n",
    "xgb_predictor.serializer = CSVSerializer() \n",
    "\n",
    "# Make predictions\n",
    "predictions = xgb_predictor.predict(test_array)\n",
    "\n",
    "# If predictions come back as bytes, decode and clean them\n",
    "if isinstance(predictions, bytes):\n",
    "    predictions = predictions.decode('utf-8').strip()\n",
    "\n",
    "# Convert the prediction string into a NumPy array\n",
    "predictions_array = np.fromstring(predictions, sep=',')\n",
    "print(predictions_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3390cbf-ed38-4d31-a3cc-d28d3fc28330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
